{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "#from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15998, 99) (4002, 99)\n"
     ]
    }
   ],
   "source": [
    "tr = '../Data/train.csv'\n",
    "ts = '../Data/test.csv'\n",
    "train_set = pd.read_csv(tr)\n",
    "test_set  = pd.read_csv(ts)\n",
    "\n",
    "# print ('Training set')\n",
    "# print (train_set.head())\n",
    "# print ('\\nTest set')\n",
    "# print (test_set.head())\n",
    "# print ('\\nOriginal DataFrame')\n",
    "# print (data.head())\n",
    "\n",
    "print(train_set.shape,test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.iloc[:,:93]\n",
    "Y_train = train_set.iloc[:,93:]\n",
    "\n",
    "X_test  = test_set.iloc[:,:93]\n",
    "Y_test  = test_set.iloc[:,93:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (93, 15998)\n",
      "Y_train shape: (6, 15998)\n",
      "X_test shape: (93, 4002)\n",
      "Y_test shape: (6, 4002)\n",
      "                                              0         1         2     \\\n",
      "Acedamic percentage in Operating Systems  1.091866  0.396841  0.694709   \n",
      "percentage in Algorithms                 -1.683226  0.099190 -0.296902   \n",
      "Percentage in Programming Concepts        0.683037 -1.388474 -0.993901   \n",
      "Percentage in Software Engineering       -0.011203 -0.011203 -0.011203   \n",
      "Percentage in Computer Networks           1.601635 -0.195059  0.004573   \n",
      "\n",
      "                                              3         4         5     \\\n",
      "Acedamic percentage in Operating Systems  0.098973 -1.688235 -0.993210   \n",
      "percentage in Algorithms                 -0.296902  1.584537  1.584537   \n",
      "Percentage in Programming Concepts       -1.585761  0.091176 -0.895257   \n",
      "Percentage in Software Engineering        1.475688 -1.002463 -0.605959   \n",
      "Percentage in Computer Networks           0.603472  1.202370  0.603472   \n",
      "\n",
      "                                              6         7         8     \\\n",
      "Acedamic percentage in Operating Systems  1.489023 -0.893920  0.893287   \n",
      "percentage in Algorithms                  1.683560  0.099190  0.594306   \n",
      "Percentage in Programming Concepts        0.387106 -0.204754  0.880323   \n",
      "Percentage in Software Engineering       -0.011203  1.376562  0.385302   \n",
      "Percentage in Computer Networks           0.304023  1.002737 -1.392856   \n",
      "\n",
      "                                              9       ...         3992  \\\n",
      "Acedamic percentage in Operating Systems  0.496130    ...    -0.596052   \n",
      "percentage in Algorithms                  1.188444    ...     0.594306   \n",
      "Percentage in Programming Concepts        1.176254    ...     0.485750   \n",
      "Percentage in Software Engineering       -0.506833    ...     1.673940   \n",
      "Percentage in Computer Networks           0.603472    ...     1.402003   \n",
      "\n",
      "                                              3993      3994      3995  \\\n",
      "Acedamic percentage in Operating Systems -0.000317  0.198262 -0.993210   \n",
      "percentage in Algorithms                  1.386491 -1.485179 -0.296902   \n",
      "Percentage in Programming Concepts       -0.303397  0.880323 -1.191188   \n",
      "Percentage in Software Engineering       -1.498094  0.980058 -1.498094   \n",
      "Percentage in Computer Networks          -1.093407  1.601635  0.403839   \n",
      "\n",
      "                                              3996      3997      3998  \\\n",
      "Acedamic percentage in Operating Systems  1.191155  0.595419  0.297551   \n",
      "percentage in Algorithms                  0.990398 -0.990064 -0.792018   \n",
      "Percentage in Programming Concepts       -0.106110 -1.487118  0.584393   \n",
      "Percentage in Software Engineering       -1.498094 -1.398967  0.980058   \n",
      "Percentage in Computer Networks           0.503655  0.403839  0.503655   \n",
      "\n",
      "                                              3999      4000      4001  \n",
      "Acedamic percentage in Operating Systems  0.595419 -0.893920 -0.397474  \n",
      "percentage in Algorithms                 -0.692995  0.990398  0.000167  \n",
      "Percentage in Programming Concepts        0.288463  1.373540 -0.303397  \n",
      "Percentage in Software Engineering        0.980058  1.079184  0.682680  \n",
      "Percentage in Computer Networks          -1.293039 -1.093407 -0.694141  \n",
      "\n",
      "[5 rows x 4002 columns]\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset \n",
    "# X_train = X_train.T\n",
    "# Y_train = Y_train.T\n",
    "# X_test  = X_test.T\n",
    "# Y_test  = Y_test.T\n",
    "\n",
    "m_train = X_train.shape[1] #no of train samples\n",
    "n       = X_train.shape[0] #no of train features\n",
    "m_test  = Y_test.shape[1] #no of test samples\n",
    "\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "print(X_test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the marks columns (1-9)\n",
    "# myu = [77.02508004268944,  76.95090715048026,  77.03868729989328,  77.13046958377801, 76.94223585912486,  77.0510272145144,   76.92062433297758,  76.91355389541089, 7.981456776947706,  5.005803094983992,  2.974919957310566,  5.009271611526147, 5.001867662753469]\n",
    "# sig = [10.069482344113009, 10.100838612474481, 10.155240754077084, 10.10313965184933, 10.008561395008378, 10.105803974768442, 10.159253374201388, 10.09967655273072, 2.5876773324068223, 2.5826669245403986, 2.0067022963210617, 2.576243318124351, 2.577799489969915]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_placeholders\n",
    "#print(X_train.head())\n",
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, no of features (93)\n",
    "    n_y -- scalar, number of classes (34)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(dtype = \"float32\" , shape = (n_x,None) , name=\"X\")\n",
    "    Y = tf.placeholder(dtype = \"float32\" , shape = (n_y,None) , name=\"Y\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"X:0\", shape=(12288, ?), dtype=float32)\n",
      "Y = Tensor(\"Y:0\", shape=(6, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Tesing\n",
    "X, Y = create_placeholders(12288, 6)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [50, 93]\n",
    "                        b1 : [50, 1]\n",
    "                        W2 : [45, 50]\n",
    "                        b2 : [45, 1]\n",
    "                        W3 : [40, 45]\n",
    "                        b3 : [40, 1]\n",
    "                        W4 : [38, 40]\n",
    "                        b4 : [38, 1]\n",
    "                        W5 : [36, 38]\n",
    "                        b5 : [36, 1]\n",
    "                        W6 : [34, 36]\n",
    "                        b6 : [34, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "    W1 = tf.get_variable(\"W1\", [60, 93], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [60, 1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [60, 60], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [60, 1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [60, 60], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [60, 1], initializer = tf.zeros_initializer())\n",
    "    W4 = tf.get_variable(\"W4\", [60, 60], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b4 = tf.get_variable(\"b4\", [60, 1], initializer = tf.zeros_initializer())\n",
    "    W5 = tf.get_variable(\"W5\", [30, 60], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b5 = tf.get_variable(\"b5\", [30, 1], initializer = tf.zeros_initializer())\n",
    "    W6 = tf.get_variable(\"W6\", [6, 30], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b6 = tf.get_variable(\"b6\", [6, 1], initializer = tf.zeros_initializer())\n",
    "#     W7 = tf.get_variable(\"W7\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b7 = tf.get_variable(\"b7\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W8 = tf.get_variable(\"W8\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b8 = tf.get_variable(\"b8\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W9 = tf.get_variable(\"W9\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b9 = tf.get_variable(\"b9\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W10 = tf.get_variable(\"W10\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b10 = tf.get_variable(\"b10\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W11 = tf.get_variable(\"W11\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b11 = tf.get_variable(\"b11\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W12 = tf.get_variable(\"W12\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b12 = tf.get_variable(\"b12\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W13 = tf.get_variable(\"W13\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b13 = tf.get_variable(\"b13\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W14 = tf.get_variable(\"W14\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b14 = tf.get_variable(\"b14\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W15 = tf.get_variable(\"W15\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b15 = tf.get_variable(\"b15\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W16 = tf.get_variable(\"W16\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b16 = tf.get_variable(\"b16\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W17 = tf.get_variable(\"W17\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b17 = tf.get_variable(\"b17\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W18 = tf.get_variable(\"W18\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b18 = tf.get_variable(\"b18\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W19 = tf.get_variable(\"W19\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b19 = tf.get_variable(\"b19\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W20 = tf.get_variable(\"W20\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b20 = tf.get_variable(\"b20\", [8, 1], initializer = tf.zeros_initializer())\n",
    "#     W21 = tf.get_variable(\"W21\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b21 = tf.get_variable(\"b21\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W22 = tf.get_variable(\"W22\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b22 = tf.get_variable(\"b22\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W23 = tf.get_variable(\"W23\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b23 = tf.get_variable(\"b23\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W24 = tf.get_variable(\"W24\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b24 = tf.get_variable(\"b24\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W25 = tf.get_variable(\"W25\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b25 = tf.get_variable(\"b25\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W26 = tf.get_variable(\"W26\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b26 = tf.get_variable(\"b26\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W27 = tf.get_variable(\"W27\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b27 = tf.get_variable(\"b27\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W28 = tf.get_variable(\"W28\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b28 = tf.get_variable(\"b28\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W29 = tf.get_variable(\"W29\", [38, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b29 = tf.get_variable(\"b29\", [38, 1], initializer = tf.zeros_initializer())\n",
    "#     W30 = tf.get_variable(\"W30\", [36, 38], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b30 = tf.get_variable(\"b30\", [36, 1], initializer = tf.zeros_initializer())\n",
    "#     W31 = tf.get_variable(\"W31\", [34, 36], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b31 = tf.get_variable(\"b31\", [34, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\"b1\": b1,\"W2\": W2,\"b2\": b2,\"W3\": W3,\"b3\": b3,\"W4\": W4,\"b4\": b4,\n",
    "                  \"W5\": W5,\"b5\": b5,\n",
    "                  \"W6\": W6,\"b6\": b6,\n",
    "#                   \"W7\": W7,\"b7\": b7,\"W8\": W8,\"b8\": b8,\n",
    "#                   \"W9\": W9,\"b9\": b9,\"W10\": W10,\"b10\": b10,\n",
    "#                   \"W11\": W11,\"b11\": b11,\"W12\": W12,\"b12\": b12,\n",
    "#                   \"W13\":W13,\"b13\":b13,\"W14\": W14,\"b14\": b14,\"W15\": W15,\"b15\": b15,\n",
    "#                   \"W16\": W16,\"b16\": b16,\n",
    "#                   \"W17\":W17,\"b17\":b17,\"W18\": W18,\"b18\": b18,\"W19\": W19,\"b19\": b19,\"W20\": W20,\"b20\": b20,\n",
    "#                   \"W21\":W21,\"b21\":b21,\"W22\":W22,\"b22\":b22,\"W23\":W23,\"b23\":b23,\"W24\":W24,\"b24\":b24,\n",
    "#                   \"W25\":W25,\"b25\":b25,\"W26\":W26,\"b26\":b26,\"W27\":W27,\"b27\":b27,\"W28\":W28,\"b28\":b28,\n",
    "#                   \"W29\":W29,\"b29\":b29,\"W30\":W30,\"b30\":b30,\"W31\":W31,\"b31\":b31\n",
    "#                  \n",
    "                 }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'W19'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b986eedb1a31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W19 = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W19\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b19 = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b19\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W20 = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W20\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'W19'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters()\n",
    "    print(\"W19 = \" + str(parameters[\"W19\"]))\n",
    "    print(\"b19 = \" + str(parameters[\"b19\"]))\n",
    "    print(\"W20 = \" + str(parameters[\"W20\"]))\n",
    "    print(\"b20 = \" + str(parameters[\"b20\"]))\n",
    "#     print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "#     print(\"b3 = \" + str(parameters[\"b3\"]))\n",
    "#     print(\"W4 = \" + str(parameters[\"W4\"]))\n",
    "#     print(\"b4 = \" + str(parameters[\"b4\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "    W5 = parameters['W5']\n",
    "    b5 = parameters['b5']\n",
    "    W6 = parameters['W6']\n",
    "    b6 = parameters['b6']\n",
    "#     W7 = parameters['W7']\n",
    "#     b7 = parameters['b7']\n",
    "#     W8 = parameters['W8']\n",
    "#     b8 = parameters['b8']\n",
    "#     W9 = parameters['W9']\n",
    "#     b9 = parameters['b9']\n",
    "#     W10 = parameters['W10']\n",
    "#     b10 = parameters['b10']\n",
    "#     W11 = parameters['W11']\n",
    "#     b11 = parameters['b11']\n",
    "#     W12 = parameters['W12']\n",
    "#     b12 = parameters['b12']\n",
    "#     W13 = parameters['W13']\n",
    "#     b13 = parameters['b13']\n",
    "#     W14 = parameters['W14']\n",
    "#     b14 = parameters['b14']\n",
    "#     W15 = parameters['W15']\n",
    "#     b15 = parameters['b15']\n",
    "#     W16 = parameters['W16']\n",
    "#     b16 = parameters['b16']\n",
    "#     W17 = parameters['W17']\n",
    "#     b17 = parameters['b17']\n",
    "#     W18 = parameters['W18']\n",
    "#     b18 = parameters['b18']\n",
    "#     W19 = parameters['W19']\n",
    "#     b19 = parameters['b19']\n",
    "#     W20 = parameters['W20']\n",
    "#     b20 = parameters['b20']\n",
    "#     W21 = parameters['W21']\n",
    "#     b21 = parameters['b21']\n",
    "#     W22 = parameters['W22']\n",
    "#     b22 = parameters['b22']\n",
    "#     W23 = parameters['W23']\n",
    "#     b23 = parameters['b23']\n",
    "#     W24 = parameters['W24']\n",
    "#     b24 = parameters['b24']\n",
    "#     W25 = parameters['W25']\n",
    "#     b25 = parameters['b25']\n",
    "#     W26 = parameters['W26']\n",
    "#     b26 = parameters['b26']\n",
    "#     W27 = parameters['W27']\n",
    "#     b27 = parameters['b27']\n",
    "#     W28 = parameters['W28']\n",
    "#     b28 = parameters['b28']\n",
    "#     W29 = parameters['W29']\n",
    "#     b29 = parameters['b29']\n",
    "#     W30 = parameters['W30']\n",
    "#     b30 = parameters['b30']\n",
    "#     W31 = parameters['W31']\n",
    "#     b31 = parameters['b31']\n",
    "    \n",
    "    \n",
    "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
    "    keep_prob = 0.5\n",
    "    Z1 = tf.add(tf.matmul(tf.nn.dropout(W1, keep_prob),X),b1)                        # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(tf.nn.dropout(W2, keep_prob),A1),b2)                       # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(tf.nn.dropout(W3, keep_prob),A2),b3)                       # Z3 = np.dot(W3,a2) + b3\n",
    "    A3 = tf.nn.relu(Z3)                                  # A3 = relu(Z3)\n",
    "    Z4 = tf.add(tf.matmul(tf.nn.dropout(W4, keep_prob),A3),b4)                       # Z4 = np.dot(W4,a3) + b4\n",
    "    A4 = tf.nn.relu(Z4)                                    # A4 = relu(Z4)\n",
    "    Z5 = tf.add(tf.matmul(tf.nn.dropout(W5, keep_prob),A4),b5)                       # Z5 = np.dot(W5,a4) + b5\n",
    "    A5 = tf.nn.relu(Z5)                                    # A5 = relu(Z5)\n",
    "    Z6 = tf.add(tf.matmul(tf.nn.dropout(W6, keep_prob),A5),b6)                       # Z6 = np.dot(W6,a5) + b6\n",
    "#     A6 = tf.nn.relu(Z6)                                    # A5 = relu(Z5)\n",
    "#     Z7 = tf.add(tf.matmul(W7,A6),b7)                       # Z6 = np.dot(W6,a5) + b6\n",
    "#     A7 = tf.nn.relu(Z7)                                    # A5 = relu(Z5)\n",
    "#     Z8 = tf.add(tf.matmul(W8,A7),b8)                       # Z6 = np.dot(W6,a5) + b6\n",
    "#     A8 = tf.nn.relu(Z8)                                    # A5 = relu(Z5)\n",
    "#     Z9 = tf.add(tf.matmul(W9,A8),b9)\n",
    "#     A9 = tf.nn.relu(Z9)                                    # A5 = relu(Z5)\n",
    "#     Z10 = tf.add(tf.matmul(W10,A9),b10)\n",
    "#     A10 = tf.nn.relu(Z10)                                    # A5 = relu(Z5)\n",
    "#     Z11 = tf.add(tf.matmul(W11,A10),b11)\n",
    "#     A11 = tf.nn.relu(Z11)                                    # A5 = relu(Z5)\n",
    "#     Z12 = tf.add(tf.matmul(W12,A11),b12)\n",
    "#     A12 = tf.nn.relu(Z12)                                    # A5 = relu(Z5)\n",
    "#     Z13 = tf.add(tf.matmul(W13,A12),b13)\n",
    "#     A13 = tf.nn.relu(Z13)                                    # A5 = relu(Z5)\n",
    "#     Z14 = tf.add(tf.matmul(W14,A13),b14)\n",
    "#     A14 = tf.nn.relu(Z14)                                    # A5 = relu(Z5)\n",
    "#     Z15 = tf.add(tf.matmul(W15,A14),b15)\n",
    "#     A15 = tf.nn.relu(Z15)                                    # A5 = relu(Z5)\n",
    "#     Z16 = tf.add(tf.matmul(W16,A15),b16)\n",
    "#     A16 = tf.nn.relu(Z16)                                    # A5 = relu(Z5)\n",
    "#     Z17 = tf.add(tf.matmul(W17,A16),b17)\n",
    "#     A17 = tf.nn.relu(Z17)                                    # A5 = relu(Z5)\n",
    "#     Z18 = tf.add(tf.matmul(W18,A17),b18)\n",
    "#     A18 = tf.nn.relu(Z18)                                    # A5 = relu(Z5)\n",
    "#     Z19 = tf.add(tf.matmul(W19,A18),b19)\n",
    "#     A19 = tf.nn.relu(Z19)\n",
    "#     Z20 = tf.add(tf.matmul(W20,A19),b20)\n",
    "#     A20 = tf.nn.relu(Z20)                                    # A5 = relu(Z5)\n",
    "#     Z21 = tf.add(tf.matmul(W21,A20),b21)\n",
    "#     A21 = tf.nn.relu(Z21)                                    # A5 = relu(Z5)\n",
    "#     Z22 = tf.add(tf.matmul(W22,A21),b22)\n",
    "#     A22 = tf.nn.relu(Z22)\n",
    "#     Z23 = tf.add(tf.matmul(W23,A22),b23)\n",
    "#     A23 = tf.nn.relu(Z23)                                    # A5 = relu(Z5)\n",
    "#     Z24 = tf.add(tf.matmul(W24,A23),b24)\n",
    "#     A24 = tf.nn.relu(Z24)                                    # A5 = relu(Z5)\n",
    "#     Z25 = tf.add(tf.matmul(W25,A24),b25)\n",
    "#     A25 = tf.nn.relu(Z25)                                    # A5 = relu(Z5)\n",
    "#     Z26 = tf.add(tf.matmul(W26,A25),b26)\n",
    "#     A26 = tf.nn.relu(Z26)                                    # A5 = relu(Z5)\n",
    "#     Z27 = tf.add(tf.matmul(W27,A26),b27)\n",
    "#     A27 = tf.nn.relu(Z27)                                    # A5 = relu(Z5)\n",
    "#     Z28 = tf.add(tf.matmul(W28,A27),b28)\n",
    "#     A28 = tf.nn.relu(Z28)                                    # A5 = relu(Z5)\n",
    "#     Z29 = tf.add(tf.matmul(W29,A28),b29)\n",
    "#     A29 = tf.nn.relu(Z29)                                    # A5 = relu(Z5)\n",
    "#     Z30 = tf.add(tf.matmul(W30,A29),b30)\n",
    "#     A30 = tf.nn.relu(Z30)                                    # A5 = relu(Z5)\n",
    "#     Z31 = tf.add(tf.matmul(W31,A30),b31)\n",
    "#     ### END CODE HERE ###\n",
    "    \n",
    "    return Z6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z12 = Tensor(\"Add_4:0\", shape=(6, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(93, 34)\n",
    "    parameters = initialize_parameters()\n",
    "    Z12 = forward_propagation(X, parameters)\n",
    "    print(\"Z12 = \" + str(Z12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z31, Y, params):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z4 -- output of forward propagation (output of the last LINEAR unit), of shape (34, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z4\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z31)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)...\n",
    "    n = len(params)//2\n",
    "    regularizer = 0\n",
    "    for x in range(n):\n",
    "        regularizer = regularizer + tf.nn.l2_loss(params['W'+str(x+1)])\n",
    "    regularizer = tf.nn.l2_loss(params['W'+str(n)])\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels) + 0.9*regularizer)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-ff98a105d657>:20: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "cost = Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(93, 34)\n",
    "    parameters = initialize_parameters()\n",
    "    Z31 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z31, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: random_mini_batches\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 512, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    k = 0\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X.iloc[:, permutation]\n",
    "    shuffled_Y = Y.iloc[:, permutation]\n",
    "#.reshape((1,m))\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X.iloc[:, k*mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y.iloc[:, k*mini_batch_size : (k+1) * mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X.iloc[:, (k+1)*mini_batch_size :  ]\n",
    "        mini_batch_Y = shuffled_Y.iloc[:, (k+1)*mini_batch_size :  ]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93, 15998)\n",
      "(6, 15998)\n"
     ]
    }
   ],
   "source": [
    "# X_train = X_train.T\n",
    "# Y_train = Y_train.T\n",
    "# X_test  = X_test.T\n",
    "# Y_test  = Y_test.T\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 1024, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a four-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 93, number of training examples = 16129)\n",
    "    Y_train -- test set, of shape (output size = 34, number of training examples = 16129)\n",
    "    X_test -- training set, of shape (input size = 93, number of training examples = 3871)\n",
    "    Y_test -- test set, of shape (output size = 34, number of test examples = 3871)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(0)                             # to keep consistent results\n",
    "    seed = 2                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y = tf.placeholder(dtype = \"float32\", shape=(n_x, None) , name=\"X\"), tf.placeholder(dtype = \"float32\", shape=(n_y,None) , name=\"Y\")\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = initialize_parameters()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    #print(\"here \",X.shape)\n",
    "    Z20 = forward_propagation(X, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z20, Y, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y, keep_prob : 0.5})\n",
    "\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z20), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "        print (\"Train Accuracy : \", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy  : \", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        print (\"Learning_rate  : \",learning_rate)\n",
    "        print (\"Batch Size     : \",minibatch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93, 15998)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'keep_prob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-3aac7fc24dbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4201\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0015\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#24,20,2.14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-68cfdfaa0c05>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs, minibatch_size, print_cost)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m### START CODE HERE ### (1 line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m#print(\"here \",X.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mZ20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-121-34d86b61262a>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m                        \u001b[0;31m# Z1 = np.dot(W1, X) + b1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m                                    \u001b[0;31m# A1 = relu(Z1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m                       \u001b[0;31m# Z2 = np.dot(W2, a1) + b2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keep_prob' is not defined"
     ]
    }
   ],
   "source": [
    "print(X_train.shape) \n",
    "parameters= model(X_train, Y_train, X_test, Y_test,num_epochs = 4201,learning_rate = 0.0015, minibatch_size = 1000)\n",
    "#24,20,2.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(label, confusion_matrix):\n",
    "    col = confusion_matrix[:, label]\n",
    "    return confusion_matrix[label, label] / col.sum()\n",
    "# 0.41639057\n",
    "# Test Accuracy  :  0.1678741\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(label, confusion_matrix):\n",
    "    row = confusion_matrix[label, :]\n",
    "    return confusion_matrix[label, label] / row.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_macro_average(confusion_matrix):\n",
    "    rows, columns = confusion_matrix.shape\n",
    "    sum_of_precisions = 0\n",
    "    for label in range(rows):\n",
    "        sum_of_precisions += precision(label, confusion_matrix)\n",
    "    return sum_of_precisions / rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_macro_average(confusion_matrix):\n",
    "    rows, columns = confusion_matrix.shape\n",
    "    sum_of_recalls = 0\n",
    "    for label in range(columns):\n",
    "        sum_of_recalls += recall(label, confusion_matrix)\n",
    "    return sum_of_recalls / columns\n",
    "\n",
    "# y_Actual = \n",
    "# df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "# confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "# sn.heatmap(confusion_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
