{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16129, 127) (3871, 127)\n"
     ]
    }
   ],
   "source": [
    "roo = './roo1.csv'\n",
    "df  = pd.read_csv(roo)\n",
    "\n",
    "data      = df.copy()\n",
    "train_set = data.sample(frac=0.80645)\n",
    "test_set  = data.drop(train_set.index)\n",
    "\n",
    "# print ('Training set')\n",
    "# print (train_set.head())\n",
    "# print ('\\nTest set')\n",
    "# print (test_set.head())\n",
    "# print ('\\nOriginal DataFrame')\n",
    "# print (data.head())\n",
    "\n",
    "print(train_set.shape,test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.iloc[:,:93]\n",
    "Y_train = train_set.iloc[:,93:]\n",
    "\n",
    "X_test  = test_set.iloc[:,:93]\n",
    "Y_test  = test_set.iloc[:,93:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 16129\n",
      "Number of testing examples : 16129\n",
      "train_set shape            : (16129, 127)\n",
      "test_set shape             : (3871, 127)\n",
      "       Acedamic percentage in Operating Systems  percentage in Algorithms  \\\n",
      "11456                                        80                        93   \n",
      "16528                                        81                        80   \n",
      "3253                                         92                        91   \n",
      "18614                                        85                        76   \n",
      "1544                                         60                        68   \n",
      "\n",
      "       Percentage in Programming Concepts  Percentage in Software Engineering  \\\n",
      "11456                                  84                                  82   \n",
      "16528                                  69                                  89   \n",
      "3253                                   70                                  83   \n",
      "18614                                  92                                  68   \n",
      "1544                                   60                                  87   \n",
      "\n",
      "       Percentage in Computer Networks  Percentage in Electronics Subjects  \\\n",
      "11456                               71                                  71   \n",
      "16528                               84                                  64   \n",
      "3253                                66                                  84   \n",
      "18614                               77                                  83   \n",
      "1544                                61                                  71   \n",
      "\n",
      "       Percentage in Computer Architecture  Percentage in Mathematics  \\\n",
      "11456                                   81                         69   \n",
      "16528                                   82                         77   \n",
      "3253                                    64                         75   \n",
      "18614                                   66                         83   \n",
      "1544                                    90                         90   \n",
      "\n",
      "       Percentage in Communication skills  Hours working per day  \\\n",
      "11456                                  86                     12   \n",
      "16528                                  67                      9   \n",
      "3253                                   65                      4   \n",
      "18614                                  88                      6   \n",
      "1544                                   69                      9   \n",
      "\n",
      "           ...        Management or Technical_Management  \\\n",
      "11456      ...                                         0   \n",
      "16528      ...                                         0   \n",
      "3253       ...                                         0   \n",
      "18614      ...                                         1   \n",
      "1544       ...                                         1   \n",
      "\n",
      "       Management or Technical_Technical  Salary/work_salary  \\\n",
      "11456                                  1                   0   \n",
      "16528                                  1                   0   \n",
      "3253                                   1                   0   \n",
      "18614                                  0                   1   \n",
      "1544                                   0                   0   \n",
      "\n",
      "       Salary/work_work  hard/smart worker_hard worker  \\\n",
      "11456                 1                              1   \n",
      "16528                 1                              0   \n",
      "3253                  1                              1   \n",
      "18614                 0                              1   \n",
      "1544                  1                              1   \n",
      "\n",
      "       hard/smart worker_smart worker  worked in teams ever?_no  \\\n",
      "11456                               0                         0   \n",
      "16528                               1                         0   \n",
      "3253                                0                         0   \n",
      "18614                               0                         1   \n",
      "1544                                0                         1   \n",
      "\n",
      "       worked in teams ever?_yes  Introvert_no  Introvert_yes  \n",
      "11456                          1             0              1  \n",
      "16528                          1             1              0  \n",
      "3253                           1             1              0  \n",
      "18614                          0             1              0  \n",
      "1544                           0             1              0  \n",
      "\n",
      "[5 rows x 93 columns]\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset \n",
    "m_train = X_train.shape[0] #no of train samples\n",
    "n       = X_train.shape[1] #no of train features\n",
    "m_test  = Y_train.shape[0] #no of test samples\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples : \" + str(m_test))\n",
    "print (\"train_set shape            : \" + str(train_set.shape))\n",
    "print (\"test_set shape             : \" + str(test_set.shape))\n",
    "# Normalising the marks columns (1-9)\n",
    "myu = [77.01426002852006, 76.98474796949594, 77.07384214768429, 77.12121024242049, 76.95790191580383, 77.01605803211606, 77.05654411308822, 76.90935581871163, 76.93328786657574]\n",
    "sig = [77.67089701771499, 77.64407546751336, 77.73767831813130, 77.77726209249717, 77.60748318199020, 77.68706969992881, 77.71482937759954, 77.57664974896230, 77.59417367116924]\n",
    " #array contains variance of all train columns\n",
    "\n",
    "# for i in range(9):\n",
    "#     X_train.iloc[:,i] = (X_train.iloc[:,i] - myu[i])/sig[i]\n",
    "#     X_test.iloc[:,i] = (X_test.iloc[:,i] - myu[i])/sig[i]\n",
    "    \n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network (counting input layer)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172 -0.01072969  0.00865408]\n",
      " [-0.02301539  0.01744812 -0.00761207  0.00319039 -0.0024937 ]\n",
      " [ 0.01462108 -0.02060141 -0.00322417 -0.00384054  0.01133769]\n",
      " [-0.01099891 -0.00172428 -0.00877858  0.00042214  0.00582815]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01100619  0.01144724  0.00901591  0.00502494]\n",
      " [ 0.00900856 -0.00683728 -0.0012289  -0.00935769]\n",
      " [-0.00267888  0.00530355 -0.00691661 -0.00396754]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    #print('safe13')\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    Z = np.dot(W,A)+b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    #print('safe12')\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    #print('safe11')\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    print('safe10')\n",
    "    \"\"\"\n",
    "    Implements the softmax activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape...(size of current layer, no of samples)\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of softmax(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    exp_sum = np.sum(Z, axis=1, keepdims=True)\n",
    "    \n",
    "    A = np.divide(np.exp(Z - np.max(Z)),exp_sum)\n",
    "    \n",
    "    return A, Z   #....A => ((size of current layer, no of samples))\n",
    "\n",
    "\n",
    "def softmax_backward(dA, cache, y):\n",
    "    #print('safe9')\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    #print('here',dA.shape,y.shape)\n",
    "    dZ = dA - y \n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    #print('safe8')\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"softmax\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b) # linear_cache contains A_prev,W,b\n",
    "        A, activation_cache = softmax(Z)  # activation_cache = Z\n",
    "        #print('ac ',activation_cache)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        #print('ac ',activation_cache)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    #print('safe7')\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them,\n",
    "                indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # number of layers in the neural network (w/o input layer)\n",
    "    #print('x shape ',X.shape)\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], \"relu\")\n",
    "        #print('a ',cache)  #...cache contains a tuple (linear_cache, activation_cache)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SOFTMAX. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], \"softmax\")\n",
    "    caches.append(cache)\n",
    "    #print(caches)\n",
    "    \n",
    "    assert(AL.shape == (34,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    print('safe6')\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (no of classes, number of examples)\n",
    "    Y -- true \"label\" vector, shape (no of classes, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    cost = -np.divide(np.sum(np.multiply(Y,np.log(AL))+np.multiply(1-Y,np.log(1-AL))),m)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Compute loss from aL and y.\n",
    "    cost = -np.divide(np.sum(np.sum(np.multiply(Y,np.log(AL)))),m)\n",
    "    print('shape   ',cost.shape)\n",
    "    cost = np.squeeze(cost) # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    print('shape   ',cost.shape)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    #print('safe5')\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    #print('cc ',dZ.shape) #(34, 16129)\n",
    "    dW = (np.dot(dZ,A_prev.T))/m\n",
    "    dZ = np.array(dZ)\n",
    "    db = (np.sum(dZ,axis=1,keepdims=True))/m\n",
    "    dA_prev = (np.dot(W.T,dZ))\n",
    "    #print(db.shape,b.shape)#(34,) (34, 1)\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation, y):\n",
    "    #print('safe4')\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"softmax\" or \"relu\"\n",
    "    y -- actual predicted outcome matrix => (m,k)\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    #print(cache,len(linear_cache),len(activation_cache))\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache, y)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    #print('safe3')\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SOFTMAX group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    #print(Y.shape,AL.shape)\n",
    "    #Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL =  - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    #print('current_cache',current_cache)\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation='softmax', y=AL)\n",
    "    #dAL,current_cache[0],current_cache[1]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads['dA'+str(l+1)], current_cache, activation='relu',y=AL)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    print('safe2')\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l + 1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.00075, num_iterations = 1000, print_cost=False):#lr was 0.009\n",
    "    \n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        print('safe1')\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        print('ssss')\n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "        print('xxxxx')\n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    "        print('yyyy')\n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93, 16129)\n"
     ]
    }
   ],
   "source": [
    "# X_train = X_train.T\n",
    "# Y_train = Y_train.T\n",
    "# X_test = X_test.T\n",
    "# Y_test = Y_test.T\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "Cost after iteration 0: 0.469366\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayali/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayali/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: divide by zero encountered in log\n",
      "/home/sayali/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in multiply\n",
      "/home/sayali/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/home/sayali/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayali/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in less_equal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n",
      "yyyy\n",
      "safe2\n",
      "safe1\n",
      "safe10\n",
      "ssss\n",
      "safe6\n",
      "shape    ()\n",
      "shape    ()\n",
      "xxxxx\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-3b61803ed919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlayers_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m93\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m34\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-94-5e51bd95f265>\u001b[0m in \u001b[0;36mL_layer_model\u001b[0;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Backward propagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m### START CODE HERE ### (≈ 1 line of code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yyyy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-ba02dc07069e>\u001b[0m in \u001b[0;36mL_model_backward\u001b[0;34m(AL, Y, caches)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Initializing the backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdAL\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Another DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m             return _combine_series_frame(self, other, na_op,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_combine_frame\u001b[0;34m(self, other, func, fill_value, level)\u001b[0m\n\u001b[1;32m   4752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4753\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4754\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_arith_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4756\u001b[0m         return self._constructor(result, index=new_index, columns=new_columns,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arith_op\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m   4733\u001b[0m             \u001b[0;31m# left._binop(right, func, fill_value=fill_value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4734\u001b[0m             \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_binop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4735\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mxrav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0muse_numexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_numexpr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_bool_arith_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b, truediv, reversed, **eval_kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m                                              'b_value': b_value},\n\u001b[1;32m    110\u001b[0m                                  \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'safe'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruediv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruediv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                                  **eval_kwargs)\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'unknown type object'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numexpr/necompiler.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(ex, local_dict, global_dict, out, order, casting, **kwargs)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[0m_numexpr_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompiled_ex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mevaluate_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_ex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layers_dims = (93, 40, 40, 40, 34)\n",
    "parameters = L_layer_model(X_train, Y_train, layers_dims, num_iterations = 1000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
